\documentclass[12pt]{article}%
\input{preamble}


% Misc
\newcommand{\parafrac}[2]{\para{\frac{#1}{#2}}}

%HW 10
\newcommand{\wc}{w(\mathcal{C})}
\newcommand{\C}{\mathcal{C}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Cep}{\mathcal{C}_\epsilon}
\newcommand{\CR}{\mathcal{C}_R}
\newcommand{\sgn}[1]{\text{sgn}({#1}) }




\title{EE562 HW 2}
\author{Edward Kim}
\date{\today}

\begin{document}
\maketitle
\section{Problem 1}

\begin{enumerate}
  \item
  \begin{enumerate}
    \item For every $\omega \in (0,1)$, $X_n(\omega) = 0$ for $n > 1/\omega^{1/3}$. The only edge case remains with $\omega = 0$ as $X_n(\omega) \rightarrow \infty$ as $n \rightarrow \infty$. However, the singleton $\{0\}$ is a measure zero set in the standard unit interval probability space. Hence, $\Prob{\{\omega : \lim_{n \rightarrow \infty} X_n(\omega) = 0\}} = 1$ and $X_n \asconv 0$.

    \item Directly calculating the expectation shows that
    \[\Exp{\abs{X_n}^2} = \frac{n^2}{n^3} = \frac{1}{n}\]
    and thus $\Exp{\abs{X_n}^2} \rightarrow 0$ as $n \rightarrow \infty$. So $X_n \msconv 0$.

    \item Since $X_n \asconv 0$, it follows that $X_n \probconv 0$. To see this explicitly, for an arbitrarily small $\epsilon > 0$:

    \[\Prob{\abs{X_n} \geq \epsilon} \leq \frac{1}{n^3}
    \]
    for sufficiently large $n$. The right-hand side of the inequality vanishes as $n \rightarrow \infty$, demonstrating that $X_n \probconv 0$.

    \item

  \end{enumerate}

  \item
  \begin{enumerate}
    \item We repeat the analysis used the previous part of the problem to remark that for $\omega \in (0,1)$, $Y_n(\omega) = 0$ for $n \geq 1/\omega$. Once again, although the value of at $\omega = 0$ diverges, $\{0\}$ is a measure zero set. Hence, $Y_n \asconv 0$.

    \item Calculating the expectation reveals that:
    \[\Exp{\abs{Y_n}^2} = \frac{n^2}{n} = n \]

    This diverges as $n \rightarrow \infty$. So the sequence $X_n$ does not converge to zero in the mean square sense.

    \item By identical considerations taken in the corresponding problem of the previous part, $X_n \probconv 0$: For arbitrarily small $\epsilon >0$:

    \[ \Prob{\abs{Y_n} \geq \epsilon} \leq \frac{1}{n} \]

    for sufficiently large $n$.
  \end{enumerate}
\end{enumerate}

\section{Problem 2}
\begin{enumerate}
  \item $F_n(x) \rightarrow F(x)$ everywhere except the points of discontinuity at $x = 1, \cdots 6$. To see this, we can find the definitions of the probability mass functions:

  \begin{align*}
      F_n(x) = \begin{cases}
          0 & x < 1 + \frac{1}{n} \\
          k/6 & k + \frac{1}{n} \leq x < (k+1) + \frac{1}{n}, \quad k= 1, \cdots, 5 \\
          1 & x \geq 6 + \frac{1}{n}
        \end{cases}
  \end{align*}

  We see that for any point $y$ not on a discontinuty, $F_n(y) \rightarrow F(y)$. However, for any point at $x = 1,\cdots 6$, $F_n(x)$ does not converge to $F(x)$.

  \item $F_x(x)$ seems continuous for all positive integers $x \geq 1$.

  \item $X_n \distconv X$, as $\lim_{n \rightarrow \infty}F_n(x) = F(x)$ at a points $x$ where $F$ is continuous.
\end{enumerate}
% $.

\section{Problem 3}
To show that $X_n \distconv Y$, it suffices to show that the sequence of probability mass functions associated to the $X_n$, $f_{X_n}$ converge pointwise to probability mass function of $Y$, $f_Y$. To this end, recall that

\begin{align*}
  \Prob{X_n = k} & = {n \choose k}\left(\lambda/n\right)^k(1 - \lambda/n)^{n-k} \quad (0 \leq k \leq n) \\
  \Prob{X_n = k} & = 0 \quad (k > n)
\end{align*}

Now for sufficiently large $n$, $\frac{n!}{(n-k)!} \approx n^k$. This will allow us to perform the followiing substitution: we can first assume $k \leq n$ by choosing a sufficiently large $n$.

\[{n \choose k}\left(\lambda/n\right)^k(1 - \lambda/n)^{n-k} \approx \frac{n^k }{k!}(\lambda/n)^k \frac{(1-\lambda/n)^{n}}{(1-\lambda/n)^{k}}  = \frac{\lambda^k}{k!}\frac{(1-\lambda/n)^{n}}{(1-\lambda/n)^{k}} \rightarrow \frac{\lambda^k}{k!}e^{-\lambda}  \]
as $n \rightarrow \infty$. This shows that $f_{X_n}(k) \rightarrow f_Y(k)$ for $k = 0,1,2, \cdots$ as required.

\section{Problem 4}
First, define the sequence $Z_n = X_n + Y_n$ and the sum of the limits $Z = X + Y$. Consider any $\epsilon >0$. Note that from the triangle inequality that if $\omega \in \Omega$ such that $\abs{Z_n(\omega) - Z(\omega)} \geq 2\epsilon$

\[ 2 \epsilon \leq \abs{Z_n(\omega) - Z(\omega)} \leq \abs{X_n(\omega) - X(\omega)} + \abs{Y_n(\omega) - Y(\omega)} \]

This implies that:

\[ \{\omega : \abs{Z_n(\omega) - Z(\omega)} \geq 2\epsilon\} \subseteq \{\omega :\abs{X_n(\omega) - X(\omega)} \geq \epsilon \} \cup \{\omega :\abs{Y_n(\omega) - Y(\omega)} \geq \epsilon \}  \]

Hence, by sub-additivity
\[ \Prob{\abs{Z_n(\omega) - Z(\omega)} \geq 2\epsilon} \leq \Prob{\abs{X_n(\omega) - X(\omega)} \geq \epsilon} + \abs{Y_n(\omega) - Y(\omega)} \geq \epsilon \]

As $X_n \probconv X$ and $Y_n \probconv Y$, the two terms of the sum on the right vanish as $n \rightarrow \infty$. Hence, $Z_n \probconv Z$ as required.

\section{Problem 5}
From the $L^2$ triangle inequality:

\begin{align*}
\Exp{\abs{X_n - a}^2}^{1/2} = \Exp{\abs{(X_n - a_n) + (a_n - a)}^2}^{1/2} & \leq \Exp{\abs{(X_n - a_n)}^2}^{1/2} + \Exp{\abs{(a_n - a)}^2}^{1/2} \\
& \leq \Exp{\abs{(X_n - a_n)}^2}^{1/2} + (a_n - a)
\end{align*}

By our assumptions, $\Exp{\abs{(X_n - a_n)}^2} \rightarrow 0$ and $a_n \rightarrow a$ as $n \rightarrow \infty$. Hence, $X_n \msconv a$.

\section{Problem 6}

\begin{enumerate}
  \item For $0 < z < 1$, the distribution function and probability density functions for $Y_n$ will be
  \begin{align*}
      F_{Y_n}(z) & = 1 - (1-z)^n \\
      f_{Y_n}(z) & = n(1-z)^{n-1}
  \end{align*}

  \item
  To show mean square convergence, we compute the expected value of the second moment:
  \begin{align*}
      \Exp{Y_n^2} & = \int_0^1 z^2 f_{Y_n}(z) \; dz \\
      & = n \int_0^1 z^2(1-z)^{n-1} \; dz \\
      & = n \cdot \mathcal{B}(3,n) \\
      & = n \cdot \frac{2!\cdot(n-1)!}{(n+2)!} \\
      & = \frac{2}{(n+1)(n+2)}
  \end{align*}

  where $\mathcal{B}(a,b)$ is the beta integral with exponents $a,b$. The last term vanishes as $n \rightarrow \infty$, showing that $Y_n \msconv 0$.

  \item
  Consider an event $\omega \in (0,1)^\mathbb{N}$ corresponding to a sample of the sequence of independent events $X_n$ sampled uniformly from $(0,1)$. For $\epsilon > 0$, define the set of events:
  \[ E_{n,m} = \{Y_n \leq \frac{1}{m}\}, \quad m, n \geq 1 \]

  The probability is computed as $\Prob{E_{n,matrix}} = 1 - (1 - \epsilon)^n$. Furthermore, $E_{n,m} \subset E_{n+1,m}$. Set $E_m = \bigcup_{n=1}^\infty E_{n,m}$. By continuity, it must be that $\Prob{E_m} = \lim_{n \rightarrow \infty} \Prob{E_{n,m}} = 1$. Therefore, the set of events $\omega\in (0,1)^\mathbb{N}$ such that there exists sufficiently large $n$ such that $Y_n(\omega) \leq \frac{1}{m}$ is of probability one. Now notice that $E_{m+1} \subset E_m$. Set $E = \bigcap_{m=1}^\infty$ and invoke continuity once again:

  \[\Prob{E} = \lim_{m \rightarrow \infty} \Prob{E_m} = 1 \]

Hence, these observations culminate in that:
    \[\Prob{\{\omega:\lim_{n \rightarrow \infty} Y_n(\omega) = 0\}} = 1 \] and that $Y_n \asconv 0$.
\end{enumerate}








\end{document}
